# [Hacksteps]()
<!--Add link to hackstepsmapping...-->

# Mapping The Application  

The first step in the process of attacking an application is gathering and examining some key information about it.  

Enumerating the application's content and functionality in order to understand what the application does and how it behaves.  
Some functionalities are easy to identify while others are hidden and might require a degree of guesswork and luck to discover.  

Once the catalog of the application's functionality is ready, the next step is to closely examine every aspect of its behaviour, its core security mechanisms and the technologies being employed (on both client and server).  

This may let you identify the key attack surface and the most interesting areas where you should look for exploitable vulnerabilities.  

## Enumerating Content and Functionality

The majority of the content and functionality can be identified via manual browsing, starting from the main page and following every link, and navigating through all multistage functions (such as user registration or password resetting).  
If the application contains a _sitemap_, that is a useful starting point for enumerating content.  

However, more advanced techniques using tools must be employed for a full analysis.  

## Web Spidering

Automatic web spidering is performed by tools like Burpsuite, OWASP ZAP, ffuf... these tools request web pages, parse for links, request these links... they also performs HTML parsing looking for forms, submitting random or specified values.  

Some tools look for the file _robots.txt_ which may contain a list of URLs that search engines and web spiders are not intended to ever reach or index.  

The use of fully automated tools has some significant limitations:  
- unusual navigation machanisms (such as menus dynamically created and handled using complicated Javascript code) often are not handled proprerly
- links buried within compiled client-side objects may not be found
- multistage functionality where proper input validation is implemented may limit the efficiency of these tools because they are not capable to understand how to compile forms
- usually spidering tools store URLs as unique entries in order to not spider the target indefinitely, applications which rely on the POST method to communicate data to the back-end server using one URL may well limit these tools
- resources placed behind an authentication form may remain undiscovered, in this case already available cookies are needed but may not be enough:
    - by following URLs, these tools may eventually hit the log-out link
    - if the spider submits an invalid input, the application may defensively terminate the session

In some applications, even running a simple web spidering tool can be extremely dangerous. An application may contain administrative functionality that deletes users, shuts down a database, restarts the server and so on...  
For example, a spidering tool may find access to a CMS (Content Management System) and from that an 'edit page' functionality to which the tool will send random string... meaning the main website will be defaced in real-time.  

## User-Directed Spidering

This approach requires the user to manually browse the website using a simple browser, all requests are captured in a proxy-capable tool such as Burpsuite.  

This technique offers numerous benefits:  
- unusual or complex mechanisms for navigation can be easily followed by the user
- the user controls all data submitted to the application
- the user ensures that the autheticated sessions will not end prematurely and can repeat the authentication process as needed
- any dangerous functionality is fully enumerated but the user can decide whether request it or not

The site map generated by the proxy/spider tool contains a wealth of information about the target application, which will be useful later in identifying the various attack surfaces exposed by the application.  

## Discovering Hidden Content

The correct mix of automated techniques, manual techniques and luck is the key when looking for hidden content such as:
- backup copies of live files with a non-executable extension which allows for source code analysis
- backup archives that contain a full snapshot of files
- new functionality that has been deployed but not yet linked to the main application
- default application functionality in an off-the-shelf application that has been superficially hidden from users
- old versions of files that have not been removed from the server
- configuration files containing sensitive data such as database credentials
- source files from which the application functionalities have been compiled
- comments in source code that may contain sensitive information (such as usernames, passwords, or keywords like "testing")
- log files which, again, may contain sensitive information

### Brute-Force Techniques

Automated technique which repeats the same task (or request) until the given list of entries to test is empty.  
As little defense against brute-forcing, applications may return customized error messages such as a 200 OK status code but a textual error message... or a non-200 status code for existing resources.  

- **302 Found**: if the redirect is to a login page, then the resource may be accessible only to authenticated users. If the redirect is to an error message, this may indicate a different reason. **If the redirect is to another location, this my be part of the application's logic and needed more investigation**
- **400 Bad Request**: the application may use a custom naming scheme for directories and files within URLs, or the wordlist contains some whitespaces or invalid syntax
- **401 Unauthorized** or **403 Forbidden**: this usually means the resource exists but may not be accessed by any user
- **500 Bad Request**: during content discovery, this usually means the application expects certain parameters to be submitted when requesting the resource 

### Inference (dedurre) from Published Content

Most applications use a naming scheme to identify resources and from already discovered content an attacker may fine-tune his wordlist.  

### Use of Public Information

The application may contain content and functionality that are not linked from the main content but have been linked in the past. It is likely that various historical repositories will still contain references to the hidden content.  

Two main tools can be used in this case:
- **Search engines**: such as Google, Yahoo and MSN. These maintain an index of all content that their spiders have discovered and also cached copies of much of this content
- **Web archives**: such as the WayBackMachine located at www.archive.org. These records hold browsable snapshots of websites collected across many years

Another public source of useful information about the target application is any posts that developers and others have made to Internet forums. Often, items posted on technical forums may contain information about application's technical details.

### Leveraging the Web Server

The web server itself may contain vulnerabilites which may help in discovering content and functionality not linked within the web application itself.  
Many application servers ships with default content that may help in attacking them, such as diagnostic scripts which may contain known vulnerabilities or common third-party components like shoppin carts, discussion forums and content management systems (CMSes).  

Automated tools like Wikto and Nikto can find unlinked resources performing a huge amount of tests and requests but may generate false positives or false negatives.  

Always mix manual and automated scans to increase "accuracy".  

## Application Pages Versus Functional Paths

The structure of URL requests reveals the application logic behind functionalities. Automated tools can easily spider required parameters within REST-style URLs since URL file paths contain strings that function as parameter values.  

In other cases, applications do not follow this structure-representation-by-pages but adhere to a representation based on functional paths.  
Requests are single URL pointing to a filename instructing the application about the function to use (example, register.php), the method to use, and parameters to use passed as additional information in the body of the requests.  
Automated spidering tools will read this as a simple and single URL ignoring parameters, result is an incomplete map of the application.  

Manual analysis of these functional paths helps a lot in revealing the application's logic. The logical relationships and dependencies between different functions may not correspond to the directory structure used within URLs. These logical relationships are of most interest to understand the application's core functionality and to formulate possible attacks.  

In this way it is simpler to understand expectations and assumptions of the application's developers, maybe even enabling you to find ways for breaking the logic of the function/application.  

## Discovering Hidden Parameters

Sometimes applications may use hidden parameters to control the application's logic in significant ways.  
For example, and aplication may behave differently if the parameter `bedug=true` is added to the query string of any URL. it might turn off certain input validation checks, allow the user to bypass certain access controls, or display verbose debug information in its response.  
 In many cases, the fact that the application supports this parameter cannot be inferred from any of its content.

## Analyzing the application

While mapping the application, other than enumerating its content, it is also very important to analyze the application's funcionality, behavior and technologies employed to identify hte key attack surfaces it exposes:
- Investigate the application's core functionality, the actions that can be leveraged to perform when used as intended
- Investigate off-site links, error messages, administrative and logging functions and the use of redirects
- Investigate the core security mechanisms and how they function, such as management of session state, access controls and authentication and supporting logic (user registration, password change, account recovery)
- Investigate all the locations at which the application processes user-supplied input, every URL query string parameter, item of POST data and cookie
- Investigate the technologies employed client-side, including forms, client-side scripts, thick-client components and cookies
- Investigate any other details that may gleaned about the internal structure and functionality of th server-side application

### Identify Entry Points for User Input

Key locations to pay attention to:
- Every URL string up to the query string marker
- Every parameter submitted within the URL query string
- Every parameter submitted within the body of a POST request
- Every cookie
- Every other HTTP header that the application might process, such as the _User-Agent_, _Referer_, _Accept_, _Accept-Language_, _Host_ headers.

### Request Parameters
