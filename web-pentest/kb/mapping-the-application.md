# Mapping The Application  

The first step in the process of attacking an application is gathering and examining some key information about it.  

Enumerating the application's content and functionality in order to understand what the application does and how it behaves.  
Some functionalities are easy to identify while others are hidden and might require a degree of guesswork and luck to discover.  

Once the catalog of the application's functionality is ready, the next step is to closely examine every aspect of its behaviour, its core security mechanisms and the technologies being employed (on both client and server).  

This may let you identify the key attack surface and the most interesting areas where you should look for exploitable vulnerabilities.  

## Enumerating Content and Functionality

The majority of the content and functionality can be identified via manual browsing, starting from the main page and following every link, and navigating through all multistage functions (such as user registration or password resetting).  
If the application contains a _sitemap_, that is a useful starting point for enumerating content.  

However, more advanced techniques using tools must be employed for a full analysis.  

## Web Spidering

Automatic web spidering is performed by tools like Burpsuite, OWASP ZAP, ffuf... these tools request web pages, parse for links, request these links... they also performs HTML parsing looking for forms, submitting random or specified values.  

Some tools look for the file _robots.txt_ which may contain a list of URLs that search engines and web spiders are not intended to ever reach or index.  

The use of fully automated tools has some significant limitations:  
- unusual navigation machanisms (such as menus dynamically created and handled using complicated Javascript code) often are not handled proprerly
- links buried within compiled client-side objects may not be found
- multistage functionality where proper input validation is implemented may limit the efficiency of these tools because they are not capable to understand how to compile forms
- usually spidering tools store URLs as unique entries in order to not spider the target indefinitely, applications which rely on the POST method to communicate data to the back-end server using one URL may well limit these tools
- resources placed behind an authentication form may remain undiscovered, in this case already available cookies are needed but may not be enough:
    - by following URLs, these tools may eventually hit the log-out link
    - if the spider submits an invalid input, the application may defensively terminate the session

In some applications, even running a simple web spidering tool can be extremely dangerous. An application may contain administrative functionality that deletes users, shuts down a database, restarts the server and so on...  
For example, a spidering tool may find access to a CMS (Content Management System) and from that an 'edit page' functionality to which the tool will send random string... meaning the main website will be defaced in real-time.  

## User-Directed Spidering

This approach requires the user to manually browse the website using a simple browser, all requests are captured in a proxy-capable tool such as Burpsuite.  

This technique offers numerous benefits:  
- unusual or complex mechanisms for navigation can be easily followed by the user
- the user controls all data submitted to the application
- the user ensures that the autheticated sessions will not end prematurely and can repeat the authentication process as needed
- any dangerous functionality is fully enumerated but the user can decide whether request it or not

The site map generated by the proxy/spider tool contains a wealth of information about the target application, which will be useful later in identifying the various attack surfaces exposed by the application.  

## Discovering Hidden Content

The correct mix of automated techniques, manual techniques and luck is the key when looking for hidden content such as:
- backup copies of live files with a non-executable extension which allows for source code analysis
- backup archives that contain a full snapshot of files
- new functionality that has been deployed but not yet linked to the main application
- default application functionality in an off-the-shelf application that has been superficially hidden from users
- old versions of files that have not been removed from the server
- configuration files containing sensitive data such as database credentials
- source files from which the application functionalities have been compiled
- comments in source code that may contain sensitive information (such as usernames, passwords, or keywords like "testing")
- log files which, again, may contain sensitive information

### Brute-Force Techniques

Automated technique which repeats the same task (or request) until the given list of entries to test is empty.  
As little defense against brute-forcing, applications may return customized error messages such as a 200 OK status code but a textual error message... or a non-200 status code for existing resources.  

- **302 Found**: if the redirect is to a login page, then the resource may be accessible only to authenticated users. If the redirect is to an error message, this may indicate a different reason. **If the redirect is to another location, this my be part of the application's logic and needed more investigation**
- **400 Bad Request**: the application may use a custom naming scheme for directories and files within URLs, or the wordlist contains some whitespaces or invalid syntax
- **401 Unauthorized** or **403 Forbidden**: this usually means the resource exists but may not be accessed by any user
- **500 Bad Request**: during content discovery, this usually means the application expects certain parameters to be submitted when requesting the resource 

### Inference (dedurre) from Published Content

Most applications use a naming scheme to identify resources and from already discovered content an attacker may fine-tune his wordlist.  

### Use of Public Information

The application may contain content and functionality that are not linked from the main content but have been linked in the past. It is likely that various historical repositories will still contain references to the hidden content.  

Two main tools can be used in this case:
- **Search engines**: such as Google, Yahoo and MSN. These maintain an index of all content that their spiders have discovered and also cached copies of much of this content
- **Web archives**: such as the WayBackMachine located at www.archive.org. These records hold browsable snapshots of websites collected across many years

Another public source of useful information about the target application is any posts that developers and others have made to Internet forums. Often, items posted on technical forums may contain information about application's technical details.

### Leveraging the Web Server

The web server itself may contain vulnerabilites which may help in discovering content and functionality not linked within the web application itself.  
Many application servers ships with default content that may help in attacking them, such as diagnostic scripts which may contain known vulnerabilities or common third-party components like shoppin carts, discussion forums and content management systems (CMSes).  

Automated tools like Wikto and Nikto can find unlinked resources performing a huge amount of tests and requests but may generate false positives or false negatives.  

Always mix manual and automated scans to increase "accuracy".  

## Application Pages Versus Functional Paths

The structure of URL requests reveals the application logic behind functionalities. Automated tools can easily spider required parameters within REST-style URLs since URL file paths contain strings that function as parameter values.  

In other cases, applications do not follow this structure-representation-by-pages but adhere to a representation based on functional paths.  
Requests are single URL pointing to a filename instructing the application about the function to use (example, register.php), the method to use, and parameters to use passed as additional information in the body of the requests.  
Automated spidering tools will read this as a simple and single URL ignoring parameters, result is an incomplete map of the application.  

Manual analysis of these functional paths helps a lot in revealing the application's logic. The logical relationships and dependencies between different functions may not correspond to the directory structure used within URLs. These logical relationships are of most interest to understand the application's core functionality and to formulate possible attacks.  

In this way it is simpler to understand expectations and assumptions of the application's developers, maybe even enabling you to find ways for breaking the logic of the function/application.  

## Discovering Hidden Parameters

Sometimes applications may use hidden parameters to control the application's logic in significant ways.  
For example, and application may behave differently if the parameter `debug=true` is added to the query string of any URL. It might turn off certain input validation checks, allow the user to bypass certain access controls, or display verbose debug information in its response.  
 In many cases, the fact that the application supports this parameter cannot be inferred from any of its content.

## Analyzing the application

While mapping the application, other than enumerating its content, it is also very important to analyze the application's functionality, behavior and technologies employed to identify the key attack surfaces it exposes:
- Investigate the application's core functionality, the actions that can be leveraged to perform when used as intended
- Investigate off-site links, error messages, administrative and logging functions and the use of redirects
- Investigate the core security mechanisms and how they function, such as management of session state, access controls and authentication and supporting logic (user registration, password change, account recovery)
- Investigate all the locations at which the application processes user-supplied input, every URL query string parameter, item of POST data and cookie
- Investigate the technologies employed client-side, including forms, client-side scripts, thick-client components and cookies
- Investigate any other details that may gleaned about the internal structure and functionality of th server-side application

### Identify Entry Points for User Input

Key locations to pay attention to:
- Every URL string up to the query string marker
- Every parameter submitted within the URL query string
- Every parameter submitted within the body of a POST request
- Every cookie
- Every other HTTP header that the application might process, such as the _User-Agent_, _Referer_, _Accept_, _Accept-Language_, _Host_ headers.

### Request Parameters

Some applications may use nonstandard parameter formats:
- /dir/file;foo=bar&foo2=bar2
- /dir/file?foo=bar$foo2=bar2
- /dir/file/foo%3dbar%26foo2%3dbar2  
`/dir/file/foo=bar&foo2=bar2`
- /dir/foo.bar/file
- /dir/foo=bar/file
- /dir/file?param=foo:bar
- /dir/file?data=%3cfoo%3ebar%3c%2ffoo%3e%3cfoo2%3ebar2%3c%2ffoo2%3e  
`/dir/file?data=<foo>bar</foo><foo2>bar2</foo2>`  

### HTTP Headers

HTTP Referer and User-Agent headers can be used to bypass applications controls. The Referer header is used to present tailored messages to users coming from different sources.  

The User-Agent header is used by applications to present different versions of UI to users using different devices. Sending requests using different User-Agent headers and analyzing different UIs can be useful for finding vulnerabilities.  

Applications residing behind a load balancer or proxy may use the IP address specified in the `X-Forwarded-For` request header (if present).  
Developers may then mistakenly assume that the IP address value is secure and process it in dangerous ways. By adding a suitably crafted `X-Forwarded-For` header, you may be able to deliver attacks such as SQL injection or persistent cross-site scripting.

## Identifying Server-Side Technologies

### Banner Grabbing

Many web servers disclose version information about the web server software itself and about other components installed.  
For example, the HTTP `Server` header discloses ahuge amount of details.  
Other useful locations are:
- Templates used to build HTML pages
- Custom HTTP headers
- URL query string parameters

### HTTP Fingerprinting

Since most application server software allows administrators to configure the `Server` banner, attackers may rely on tools able to fingerprint the HTTP response and determine, analyzing subtle variations in responses, the underlying web sofware in use. 
**Httprecon** is a tool able to fingerprint a web server's software.  

### File Extensions

File extensions used within URLs often disclose the platform or programming language used to implement the relevant functionality. For example:
- **asp** - Microsoft Active Server Pages
- **aspx** - Microsoft ASP.NET
- **jsp** - Java Server Pages
- **cfm** - Cold Fusion
- **php** - The PHP Language
- **d2w** - WebSphere
- **pl** - The Perl Language
- **py** - The Python Language
- **dll** - Usually compiled native code (C or C++)
- **nsf** or **ntf** - Lotus Domino

Even if file extensions of supported files are not explicitly found in URLs or across the website, it is possible to trigger customized error messages (based upon the platform/language being used on the web server) requesting non existant files each with different extensions.  

For example, requesting a .aspx file may trigger a customized error message if ASP.NET is implemented on the server. Requesting files with other extensions simply return general error messages.  

Automating these kind of requests may speed up the entire process and quickly confirm technologies implemented on the server.  

### Directory Names

Sometimes it may be possible to detect subdirectories whose names indicate the presence of an associated technology:
- **servlet** - Java servlets
- **pls** - Oracle Application Server PL/SQL gateway
- **cfdocs** or **cfide** - Cold Fusion
- **SilverStream** - The SilverStream web server
- **WebObjects** or **(function).woa** - Apple WebObjects
- **rails** - Ruby on Rails

### Session Tokens

Many web server application platforms generate sessions tokens by default which may hold information about the technology in use:
- **JSESSIONID** - The Java Platform
- **ASPESESSIONID** - Microsoft IIS Server
- **ASP.NET_SessionId** - Microsoft ASP.NET
- **CFID/CFTOKEN** - Cold Fusion
- **PHPSESSID** - PHP

### Third-Party Code Components

Many web applications incorporate third-party code components to implement common functionality such as shopping carts, login mechanisms and message boards. These may be open-source or may have been purchased from an external developer.  
These components may be adopted by many other applications each using a specific set of the component's features.  
Furthermore, you may download it and test it "offline", or find already known vulnerabilities to exploit.  

## Identifying Server-Side Functionality

### Dissecting Requests

Sometimes URLs may give out more clues than developers ever intended to.  
File extension may reveal which platform/programming language is in use in the backend server while the presence of known keywords may reveal the presence of a back-end database of a specific "nature".  

### Extrapolating Application Behavior

Encoded/obfuscated input may be appear deobfuscated elsewhere in the application, like in error messages. Once the obfuscating scheme is uncovered, an attacker may use it to decode data or generate (for example) obfuscated cookies to bypass authorization checks.

## Mapping the Attack Surface

- **Client-side validation**: Checks may not be replicated on the server
- **Database interaction** : SQL Injection
- **File uploading and downloading**: Path traversal vulnerabilities, stored XSS
- **Display of user-supplied data**: XSS
- **Dynamic redirects**: Redirection and header injection attacks
- **Social networking features**: Username enumeration, stored XSS scripting
- **Login**: Username enumeration, weak passwords, ability to use brute force
- **Multistage login**: Logic flaws
- **Session state**: Predictable tokens, insecure handling of tokens
- **Access controls**: Horizontal and vertical privilege escalation
- **User impersonation functions**: Privilege escalation
- **Use of cleartext communications**: Session hijacking, capture of credentials and other sensitive data
- **Off-site links**: Leakage of query string parameters in the Referer header
- **Interfaces to external systems**: Shortcuts in the handling of sessions and/or access controls
- **Error messages**: Information leakage
- **E-mail interaction**: E-mail and/or command injection
- **Native code components or interaction**: Buffer overflow
- **Use of third-party application components**: Known vulnerabilities
- **Identifiable web-server software**: Common configuration weaknesses, known software bugs