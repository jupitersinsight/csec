## HTML

Browsers and Web-Application Firewalls and other pieces of software might parse HTML in different ways and where this happens, there well might be a vulnerability.

HTML5 introduced default behaviors to follow for browsers when parsing legacy HTML code, examples are:
- the **script** tag on its own will automatically be closed at the end of the page
-  a tag missing its closing angle bracket will automatically be closed by the angle bracket of the next tag on the page


________

## SOP (Same-Origin Policy)

SOP is how the browser restricts a number of security-critical features:
- What domains can you contact via XMLHttpRequest
- Access the DOM across separate frames/windows

Some key features of Same-Origin Policy are:
- A page residing on one domain can cause and arbitrary request to be made to another domain (for example, by submitting a form or loading an image). But it cannot itself process tha data returned from that request.
- A page residing on one domain can load a script from another domain and execute this within its context. This is because scripts are assumed to contain code, rather than data, so cross-domain access should not lead to disclosure of any sensitive information.
- A page residing on one domain cannot read or modify the cookies or other DOM data belonging to another domain.

### Origin Matching

The way origin matching for SOP works is much more strict than cookies:
- Protocol must match otherwise meaning there is no crossing HTTP/HTTPS boundaries
- Port numbers must match
- Domain names must be and exact match meaning wildcards and subdomain walking are not allowed

It is possible for developers to loosen the grip that SOP has on their communications by changing document.domain, posting messages between windows and by using CORS (Cross-Origin Resource Sharing).
Every little change opens up new avenues for attack.


________

## CORS (Cross-Origin Resource Sharing)

CORS allows XMLHtppRequests outside the origin domain but with special headers.  
They even support sending and retrieval of cookies (going up to the remote server).

________

## CSRF (Cross-Site Request Forgery)

CRSF is when an attacker tricks a victim into going to a page controlled by the attacker, which then submits data to the target site _as_ the victim.  

CSRF tokens are randomnly generated and tied to users' sessions in order to prevent POST requests coming from unauthorized users (both logged-in and not logged-in).

Missing CSRF tokens in forms, of any kind, is to be considered a vulnerability, meaning a finding.

________

## Mapping The Application  

The first step in the process of attacking an application is gathering and examining some key information about it.  

Enumerating the application's content and functionality in order to understand what the application does and how it behaves.  
Some functionalities are easy to identify while others are hidden and might require a degree of guesswork and luck to discover.  

Once the catalog of the application's functionality is ready, the next step is to closely examine every aspect of its behaviour, its core security mechanisms and the technologies being employed (on both client and server).  

This may let you identify the key attack surface and the most interesting areas where you should look for exploitable vulnerabilities.  

_______

## Enumerating Content and Funcionality

The majority of the content and functionality can be identified via manual browsing, starting from the main page and following every link, and navigating through all multistage functions (such as user registration or password resetting).  
If the application contains a _sitemap_, that is a useful starting point for enumerating content.  

However, more advanced techniques using tools must be employed for a full analysis.  

### Web Spidering

Automatic web spidering is performed by tools like Burpsuite, OWASP ZAP, ffuf... these tools request web pages, parse for links, request these links... they also performs HTML parsing looking for forms, submitting random or specified values.  

Some tools look for the file _robots.txt_ which may contain a list of URLs that search engines and web spiders are not intended to ever reach or index.  

The use of fully automated tools has some significant limitations:  
- unusual navigation machanisms (such as menus dynamically created and handled using complicated Javascript code) often are not handled proprerly
- links buried within compiled client-side objects may not be found
- multistage functionality where proper input validation is implemented may limit the efficiency of these tools because they are not capable to understand how to compile forms
- usually spidering tools store URLs as unique entries in order to not spider the target indefinitly, applications which rely on the POST method to communicate data to the back-end server using one URL may well limit these tools
- resources placed behind an authentication form may remain undiscovered, in this case already available cookies are needed but may not be enough:
    - by following URLs, these tools may eventually hit the log-out link
    - if the spider submits an invalid input, the application may defensively terminate the session

In some applications, even running a simple web spidering tool can be extremely dangerous. An application may contain administrative functionality that deletes users, shuts down a database, restarts the server and so on...  
For example, a spidering tool may find access to a CMS (Content Management System) and from that an 'edit page' functionality to which the tool will send random string... meaning the main website will be defaced in real-time.  

### User-Directed Spidering

This approach requires the user to manually browse the website using a simple browser, all requests are captured in a proxy-capable tool such as Burpsuite.  

This technique offers numerous benefits:  
- unusual or complex mechanisms for navigation can be easily followed by the user
- the user controls all data submitted to the application
- the user ensures that the autheticated sessions will not end prematurely and can repeat the authentication process as needed
- any dangerous functionality is fully enumerated but the user can decide whether request it or not

**STEPS TO FOLLOW**
- Configure the browser to send requests to the proxy/capture software
- Browse the entire application normally, attempting to visit every link/URL you discover, submitting every form, and proceeding through all multi-step functions to completion. Try browsing with Javascript enabled and disabled, and with cookies enabled and disabled. Many applications can handle various browser configurations, and you may reach different content and code paths within the application
- Review the site map generated by the proxy/spider tool, and identify any application content or function that you did not browse manually. Establish how the spider enumerated each item
- Optionally, tell the tool to actively spider the site using all of the already enumerated content as a starting point. To do this, first identify any URLs that are dangerous or likely to break the application session, and configure the spider to exclude these from its scope

The site map generated by the proxy/spider tool contains a wealth of information about the target application, which will be useful later in identifying the various attack surfaces exposed by the application.  

## Discovering Hidden Content




