## HTML

Browsers and Web-Application Firewalls and other pieces of software might parse HTML in different ways and where this happens, there well might be a vulnerability.

HTML5 introduced default behaviors to follow for browsers when parsing legacy HTML code, examples are:
- the **script** tag on its own will automatically be closed at the end of the page
-  a tag missing its closing angle bracket will automatically be closed by the angle bracket of the next tag on the page


________

## SOP (Same-Origin Policy)

SOP is how the browser restricts a number of security-critical features:
- What domains can you contact via XMLHttpRequest
- Access the DOM across separate frames/windows

Some key features of Same-Origin Policy are:
- A page residing on one domain can cause and arbitrary request to be made to another domain (for example, by submitting a form or loading an image). But it cannot itself process tha data returned from that request.
- A page residing on one domain can load a script from another domain and execute this within its context. This is because scripts are assumed to contain code, rather than data, so cross-domain access should not lead to disclosure of any sensitive information.
- A page residing on one domain cannot read or modify the cookies or other DOM data belonging to another domain.

### Origin Matching

The way origin matching for SOP works is much more strict than cookies:
- Protocol must match otherwise meaning there is no crossing HTTP/HTTPS boundaries
- Port numbers must match
- Domain names must be and exact match meaning wildcards and subdomain walking are not allowed

It is possible for developers to loosen the grip that SOP has on their communications by changing document.domain, posting messages between windows and by using CORS (Cross-Origin Resource Sharing).
Every little change opens up new avenues for attack.


________

## CORS (Cross-Origin Resource Sharing)

CORS allows XMLHtppRequests outside the origin domain but with special headers.  
They even support sending and retrieval of cookies (going up to the remote server).

### What is the Access-Control-Allow-Origin response header?

The _Access-Control-Allow-Origin_ header is included in the response from one website to a request originating from another website, and identifies the permitted origin of the request. A web browser compares the Access-Control-Allow-Origin with the requesting website's origin and permits access to the response if they match.

### Implementing simple cross-origin resource sharing

The cross-origin resource sharing (CORS) specification prescribes header content exchanged between web servers and browsers that restricts origins for web resource requests outside of the origin domain. The CORS specification identifies a collection of protocol headers of which _Access-Control-Allow-Origin_ is the most significant. This header is returned by a server when a website requests a cross-domain resource, with an _Origin_ header added by the browser.

For example, suppose a website with origin normal-website.com causes the following cross-domain request:

```http
GET /data HTTP/1.1
Host: robust-website.com
Origin : https://normal-website.com
The server on robust-website.com returns the following response:
```
```http
HTTP/1.1 200 OK
...
Access-Control-Allow-Origin: https://normal-website.com
```

The browser will allow code running on normal-website.com to access the response because the origins match.


The specification of Access-Control-Allow-Origin allows for multiple origins, or the value null, or the wildcard *. However, no browser supports multiple origins and there are restrictions on the use of the wildcard *.

### Handling cross-origin resource requests with credentials

The default behavior of cross-origin resource requests is for requests to be passed without credentials like cookies and the Authorization header.  

However, the cross-domain server can permit reading of the response when credentials are passed to it by setting the CORS Access-Control-Allow-Credentials header to true. Now if the requesting website uses JavaScript to declare that it is sending cookies with the request:

```http
GET /data HTTP/1.1
Host: robust-website.com
...
Origin: https://normal-website.com
Cookie: JSESSIONID=<value>
```

And the response to the request is:

```http
HTTP/1.1 200 OK
...
Access-Control-Allow-Origin: https://normal-website.com
Access-Control-Allow-Credentials: true
```

Then the browser will permit the requesting website to read the response, because the Access-Control-Allow-Credentials response header is set to true. Otherwise, the browser will not allow access to the response.

A cross-domain server response of the form:

```http
Access-Control-Allow-Origin: *
Access-Control-Allow-Credentials: true
```

is not permitted as this would be dangerously insecure, exposing any authenticated content on the target site to everyone.
________________________

## Mapping The Application  

The first step in the process of attacking an application is gathering and examining some key information about it.  

Enumerating the application's content and functionality in order to understand what the application does and how it behaves.  
Some functionalities are easy to identify while others are hidden and might require a degree of guesswork and luck to discover.  

Once the catalog of the application's functionality is ready, the next step is to closely examine every aspect of its behaviour, its core security mechanisms and the technologies being employed (on both client and server).  

This may let you identify the key attack surface and the most interesting areas where you should look for exploitable vulnerabilities.  

_______

## Enumerating Content and Functionality

The majority of the content and functionality can be identified via manual browsing, starting from the main page and following every link, and navigating through all multistage functions (such as user registration or password resetting).  
If the application contains a _sitemap_, that is a useful starting point for enumerating content.  

However, more advanced techniques using tools must be employed for a full analysis.  

### Web Spidering

Automatic web spidering is performed by tools like Burpsuite, OWASP ZAP, ffuf... these tools request web pages, parse for links, request these links... they also performs HTML parsing looking for forms, submitting random or specified values.  

Some tools look for the file _robots.txt_ which may contain a list of URLs that search engines and web spiders are not intended to ever reach or index.  

The use of fully automated tools has some significant limitations:  
- unusual navigation machanisms (such as menus dynamically created and handled using complicated Javascript code) often are not handled proprerly
- links buried within compiled client-side objects may not be found
- multistage functionality where proper input validation is implemented may limit the efficiency of these tools because they are not capable to understand how to compile forms
- usually spidering tools store URLs as unique entries in order to not spider the target indefinitely, applications which rely on the POST method to communicate data to the back-end server using one URL may well limit these tools
- resources placed behind an authentication form may remain undiscovered, in this case already available cookies are needed but may not be enough:
    - by following URLs, these tools may eventually hit the log-out link
    - if the spider submits an invalid input, the application may defensively terminate the session

In some applications, even running a simple web spidering tool can be extremely dangerous. An application may contain administrative functionality that deletes users, shuts down a database, restarts the server and so on...  
For example, a spidering tool may find access to a CMS (Content Management System) and from that an 'edit page' functionality to which the tool will send random string... meaning the main website will be defaced in real-time.  

### User-Directed Spidering

This approach requires the user to manually browse the website using a simple browser, all requests are captured in a proxy-capable tool such as Burpsuite.  

This technique offers numerous benefits:  
- unusual or complex mechanisms for navigation can be easily followed by the user
- the user controls all data submitted to the application
- the user ensures that the autheticated sessions will not end prematurely and can repeat the authentication process as needed
- any dangerous functionality is fully enumerated but the user can decide whether request it or not

**STEPS TO FOLLOW**
- Configure the browser to send requests to the proxy/capture software
- Browse the entire application normally, attempting to visit every link/URL you discover, submitting every form, and proceeding through all multi-step functions to completion. Try browsing with Javascript enabled and disabled, and with cookies enabled and disabled. Many applications can handle various browser configurations, and you may reach different content and code paths within the application
- Review the site map generated by the proxy/spider tool, and identify any application content or function that you did not browse manually. Establish how the spider enumerated each item
- Optionally, tell the tool to actively spider the site using all of the already enumerated content as a starting point. To do this, first identify any URLs that are dangerous or likely to break the application session, and configure the spider to exclude these from its scope

The site map generated by the proxy/spider tool contains a wealth of information about the target application, which will be useful later in identifying the various attack surfaces exposed by the application.  

## Discovering Hidden Content

The correct mix of automated techniques, manual techniques and luck is the key when looking for hidden content such as:
- backup copies of live files with a non-executable extension which allows for source code analysis
- backup archives that contain a full snapshot of files
- new functionality that has been deployed but not yet linked to the main application
- default application functionality in an off-the-shelf application that has been superficially hidden from users
- old versions of files that have not been removed from the server
- configuration files containing sensitive data such as database credentials
- source files from which the application functionalities have been compiled
- comments in source code that may contain sensitive information (such as usernames, passwords, or keywords like "testing")
- log files which, again, may contain sensitive information

### Brute-Force Techniques

Automated technique which repeats the same task (or request) until the given list of entries to test is empty.  
As little defense against brute-forcing, applications may return customized error messages such as a 200 OK status code but a textual error message... or a non-200 status code for existing resources.  

- **302 Found**: if the redirect is to a login page, then the resource may be accessible only to authenticated users. If the redirect is to an error message, this may indicate a different reason. **If the redirect is to another location, this my be part of the application's logic and needed more investigation**
- **400 Bad Request**: the application may use a custom naming scheme for directories and files within URLs, or the wordlist contains some whitespaces or invalid syntax
- **401 Unauthorized** or **403 Forbidden**: this usually means the resource exists but may not be accessed by any user
- **500 Bad Request**: during content discovery, this usually means the application expects certain parameters to be submitted when requesting the resource 

**STEPS**
1. Make some manual requests for known valid and invalid resources, take note how the server handles them
2. Use the site map generated through the user-directed spidering as basis for automated discovery
3. Make automated requests for common filenames and directories for each known path in the application, use information from step 1 in the spidering tool to highlight valid or invalid requests
4. Capture the responsed from the server and manually review them
5. Repeat for every new discovered

### Inference (dedurre) from Published Content

Most applications use a naming scheme to identify resources and from already discovered content an attacker may fine-tune his wordlist.  

**STEPS**  
1. Review the results of your user-directed spidering and automated spidering (brute-force). From those compile a list of all enumerated subdirectories, file stems and file extensions
2. Review them to identify any naming schemes (for example, known filenames _AddUser.ext_ and _DeleteUser.ext_ show the developer's naming scheme, which may lead to fine-tune the brute-force tool with the pattern *User.ext where * may be _Edit_, _Block_...)
3. In case of static resources, the naming scheme may contain dates and numbers such as _annualtext2009.ext_, _annualtext2010.ext_ and so on...
4.  Review all client-side HTML and Javascript to identify any clue to hidden server-side content
5. Add to the lists of enumerated items any further potential names based on what you have discovered so far, also add to the file extension list common extensions such as .txt, .bak, .src, .inc and .old. Also extension of development language(s) in use in the application
6. Search for temporary files that developers tools and file editors may have left behind (such as _.DS\_Store_, _file.php[--]-1 or .tmp)
7. Combine the lists of directories, file stems and file extensions to request large numbers of potential resources
8. Where a consistent naming scheme has been identified, a more focused brute-force must be performed
9. Repeat all steps recursively using the new enumerated content and patterns as the basis for further user-directed spidering and automated content discovery




