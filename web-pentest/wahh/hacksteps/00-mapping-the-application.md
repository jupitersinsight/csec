# Mapping the application

## User-Directed Spidering

1. Configure the browser to send requests to the proxy/capture software
2. Browse the entire application normally, attempting to visit every link/URL you discover, submitting every form, and proceeding through all multi-step functions to completion. Try browsing with Javascript enabled and disabled, and with cookies enabled and disabled. Many applications can handle various browser configurations, and you may reach different content and code paths within the application
3. Review the site map generated by the proxy/spider tool, and identify any application content or function that you did not browse manually. Establish how the spider enumerated each item
4. Optionally, tell the tool to actively spider the site using all of the already enumerated content as a starting point. To do this, first identify any URLs that are dangerous or likely to break the application session, and configure the spider to exclude these from its scope

The site map generated by the proxy/spider tool contains a wealth of information about the target application, which will be useful later in identifying the various attack surfaces exposed by the application.

## Brute-Force Techniques

1. Make some manual requests for known valid and invalid resources, take note how the server handles them
2. Use the site map generated through the user-directed spidering as basis for automated discovery
3. Make automated requests for common filenames and directories for each known path in the application, use information from step 1 in the spidering tool to highlight valid or invalid requests
4. Capture the responsed from the server and manually review them
5. Repeat for every new discovered

## Inference from Published Content

1. Review the results of your user-directed spidering and automated spidering (brute-force). From those compile a list of all enumerated subdirectories, file stems and file extensions
2. Review them to identify any naming schemes (for example, known filenames _AddUser.ext_ and _DeleteUser.ext_ show the developer's naming scheme, which may lead to fine-tune the brute-force tool with the pattern *User.ext where * may be _Edit_, _Block_...)
3. In case of static resources, the naming scheme may contain dates and numbers such as _annualtext2009.ext_, _annualtext2010.ext_ and so on...
4.  Review all client-side HTML and Javascript to identify any clue to hidden server-side content
5. Add to the lists of enumerated items any further potential names based on what you have discovered so far, also add to the file extension list common extensions such as .txt, .bak, .src, .inc and .old. Also extension of development language(s) in use in the application
6. Search for temporary files that developers tools and file editors may have left behind (such as _.DS\_Store_, _file.php[--]-1 or .tmp)
7. Combine the lists of directories, file stems and file extensions to request large numbers of potential resources
8. Where a consistent naming scheme has been identified, a more focused brute-force must be performed
9. Repeat all steps recursively using the new enumerated content and patterns as the basis for further user-directed spidering and automated content discovery

## Use of Public Information

1. Use several different search engines and web archives to discover what content they indexed or stored for the application you are attacking
2. When querying a search engine, you can use various advanced techniques to maximize the effectiveness of your research. The following suggestions apply to Google
    - `site:www.target.com` returns every resource within the target site the Google has a reference to
    - `site:www.target-com login` returns all the pages containing the expression `login`. In a large and complex application, this technique can be used to quickly home in on interesting resources, such as site maps, password reset functions and administrative menus
    - `link:www.target.com` returns all the pages on other websites and applications that contain a link to the target. This may include links to old content or functionality that is intended for use only by third parties such as partner links
    - `related:www.target.com` returns pages that are "similar" to the target and therefore includes a lot of irrelevant material. However, it may also discuss the target on other sites, which may be of interest
3. Perform each search not only in the default Web section of Google but also in Groups and News, which may contain different results
4. Be careful to repeat the query forcing Google to include omitted results which the search engine believed to be sufficiently similar to the others included results
5. View the cached version of interesting pages, including any content that is no longer present in the actual application or that can be accessed only after payment or authentication
6. Perform the same queries on other domain names belonging to the same organization, which may contain useful information

### Use of Public Information - Public forums

1. Compile a list containing every name and e-mail address you can discover relating to the target application and its development. This should include any known developers, names found within HTML source code, names found in the contact information section of the main company website and any names disclosed within the application itself, such as administrative stuff
2. Using the search techniques described previously, search for each identified name to find any questions and answers they have posted to Internet forums. Review any information found for clues about functionality or vulnerabilities within the target application

### Leveraging the Web Server

Several tips when running Nikto:

1. If the server might use a nonstandard location for interesting content that Nikto checks for, options `-root` and `-Cgidirs` can be used to specify alternative locations
2. If the site uses a custom "file not found" page that does not return the status code 404, option `404` can be used to specify a particular string to identify the page
3. Nikto does not perform any intelligent verification of potential issues and therefore is prone to generate false positives results, always check results manually
4. Tools like Nikto does treat IPs and FQDN as separate entities, this default behaviour may result in an incomplete spidering of the website because links (pointing to different IPs or domains) are skipped

### Application Pages Versus Functional Paths

